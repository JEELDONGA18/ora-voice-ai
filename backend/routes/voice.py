# from fastapi import APIRouter, WebSocket, WebSocketDisconnect
# import json

# from services.elevenlabs_stt import transcribe_audio
# from services.gemini import generate_response
# from services.elevenlabs_tts import stream_tts
# from services.memory import get_session_memory, save_message
# from utils.audio import pcm_chunks_to_wav

# router = APIRouter()

# @router.websocket("/voice/{session_id}")
# async def voice_ws(ws: WebSocket, session_id: str):
#     await ws.accept()

#     audio_chunks: list[bytes] = []

#     try:
#         while True:
#             message = await ws.receive()

#             # ğŸ”Œ Client disconnected
#             if message["type"] == "websocket.disconnect":
#                 print("ğŸ”Œ websocket disconnect")
#                 break

#             # ğŸ§ Binary PCM audio
#             if "bytes" in message and message["bytes"] is not None:
#                 audio_chunks.append(message["bytes"])
#                 continue

#             # ğŸ“© JSON control message
#             if "text" in message and message["text"] is not None:
#                 data = json.loads(message["text"])

#                 if data.get("type") == "audio_end":
#                     print("ğŸ›‘ audio_end received")
#                     break

#     except WebSocketDisconnect:
#         print("ğŸ”Œ client disconnected (exception)")
#         return

#     # -------------------------------
#     # ğŸ”Š AUDIO â†’ TEXT (STT)
#     # -------------------------------
#     if not audio_chunks:
#         print("âš ï¸ no audio received")
#         await ws.close()
#         return

#     print(f"ğŸ”Š received {len(audio_chunks)} chunks")

#     wav_bytes = pcm_chunks_to_wav(audio_chunks)
#     user_text = transcribe_audio(wav_bytes)

#     save_message(session_id, "user", user_text)

#     await ws.send_json({
#         "type": "user_text",
#         "text": user_text
#     })

#     # -------------------------------
#     # ğŸ¤– GEMINI
#     # -------------------------------
#     history = get_session_memory(session_id)
#     ai_text = generate_response(history)

#     save_message(session_id, "assistant", ai_text)

#     await ws.send_json({
#         "type": "ai_text",
#         "text": ai_text
#     })

#     # -------------------------------
#     # ğŸ”Š STREAM TTS BACK
#     # -------------------------------
#     for chunk in stream_tts(ai_text):
#         await ws.send_bytes(chunk)

#     await ws.send_json({ "type": "tts_end" })
#     await ws.close()

from fastapi import APIRouter, WebSocket, WebSocketDisconnect
import json

from services.elevenlabs_stt import transcribe_audio
from services.gemini import generate_response
from services.elevenlabs_tts import stream_tts
from services.memory import get_session_memory, save_message
from utils.audio import pcm_chunks_to_wav

router = APIRouter()

@router.websocket("/voice/{session_id}")
async def voice_ws(ws: WebSocket, session_id: str):
    await ws.accept()

    audio_chunks: list[bytes] = []

    try:
        while True:
            message = await ws.receive()

            # ğŸ”Œ client disconnected
            if message["type"] == "websocket.disconnect":
                print("ğŸ”Œ websocket disconnect")
                break

            # ğŸ§ binary audio
            if "bytes" in message and message["bytes"] is not None:
                audio_chunks.append(message["bytes"])
                continue

            # ğŸ“© json message
            if "text" in message and message["text"] is not None:
                data = json.loads(message["text"])

                # ğŸ›‘ SPACE RELEASE â†’ STT ONLY
                if data.get("type") == "audio_end":
                    print("ğŸ›‘ audio_end received â†’ STT only")

                    if not audio_chunks:
                        await ws.send_json({
                            "type": "user_text",
                            "text": ""
                        })
                        continue

                    wav_bytes = pcm_chunks_to_wav(audio_chunks)
                    user_text = transcribe_audio(wav_bytes)

                    save_message(session_id, "user", user_text)

                    await ws.send_json({
                        "type": "user_text",
                        "text": user_text
                    })

                    # reset buffer for next recording
                    audio_chunks.clear()
                    continue

                # âœ… ENTER KEY â†’ SUBMIT TO AI
                if data.get("type") == "submit":
                    print("ğŸš€ submit received â†’ AI + TTS")

                    history = get_session_memory(session_id)
                    ai_text = generate_response(history)

                    save_message(session_id, "assistant", ai_text)

                    await ws.send_json({
                        "type": "ai_text",
                        "text": ai_text
                    })

                    # ğŸ”Š stream TTS
                    for chunk in stream_tts(ai_text):
                        await ws.send_bytes(chunk)

                    await ws.send_json({ "type": "tts_end" })
                    continue

    except WebSocketDisconnect:
        print("ğŸ”Œ client disconnected (exception)")

    finally:
        await ws.close()
        print("ğŸ”’ websocket closed")